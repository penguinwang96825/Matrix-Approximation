{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "594c7edb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T13:15:09.949765Z",
     "start_time": "2021-12-22T13:15:09.944778Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da6a768a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T12:24:03.751234Z",
     "start_time": "2021-12-22T12:24:03.497913Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "X, y = make_classification(100, 16000, n_informative=1000, n_classes=NUM_CLASSES, random_state=914)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7dc143f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T12:24:05.654452Z",
     "start_time": "2021-12-22T12:24:05.633024Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 1, 3, 5, 8, 1, 2, 0, 0, 9, 5, 8, 2, 9, 6, 3, 4, 3, 9, 9, 3,\n",
       "       4, 0, 2, 2, 0, 6, 6, 1, 9, 3, 1, 6, 4, 7, 0, 8, 7, 1, 2, 6, 8, 0,\n",
       "       1, 0, 4, 7, 6, 7, 4, 5, 3, 7, 8, 7, 3, 7, 8, 5, 4, 7, 2, 5, 9, 4,\n",
       "       4, 8, 1, 2, 7, 8, 5, 4, 5, 9, 8, 2, 6, 3, 3, 5, 6, 0, 1, 9, 6, 8,\n",
       "       0, 7, 0, 5, 9, 4, 2, 5, 9, 1, 6, 2], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2962dade",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T12:24:29.767423Z",
     "start_time": "2021-12-22T12:24:29.760458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.24934824,  -1.2389227 ,  -0.12173659, ...,  14.727759  ,\n",
       "         -0.7366306 ,  -1.351909  ],\n",
       "       [  1.0690868 ,  -1.1818848 ,  -0.6803679 , ..., -16.5411    ,\n",
       "          0.24864252,   0.07018664],\n",
       "       [ -0.39488146,  -0.43264174,   0.5022811 , ...,   9.970029  ,\n",
       "         -0.19629164,  -0.8747166 ],\n",
       "       ...,\n",
       "       [ -1.7917298 ,   1.3505819 ,   0.046682  , ...,  16.731117  ,\n",
       "         -1.1025734 ,   1.4075642 ],\n",
       "       [ -0.8673802 ,   0.49094677,   1.4046149 , ...,   7.9407005 ,\n",
       "          0.27935445,  -0.5779842 ],\n",
       "       [ -0.44752482,  -0.56108904,  -1.0531406 , ..., -14.808728  ,\n",
       "          0.23746619,  -1.4545943 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8c00b4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T12:50:40.181974Z",
     "start_time": "2021-12-22T12:50:40.171004Z"
    }
   },
   "outputs": [],
   "source": [
    "def joint_different_speakers(waveforms, speakers, num_mix):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio_files: list\n",
    "    speakers: list\n",
    "    num_mix: int\n",
    "    \"\"\"\n",
    "    mixed = []\n",
    "    for i, wav in enumerate(waveforms):\n",
    "        current_speaker = speakers[i]\n",
    "        is_different_speakers = list(map(lambda x: x!=current_speaker, speakers))\n",
    "        different_speakers_idx = [k for k, boolean in enumerate(is_different_speakers) if boolean]\n",
    "        select_idx = list(np.random.choice(different_speakers_idx, num_mix, replace=False))\n",
    "        for j in select_idx:\n",
    "            trg_wav = wav\n",
    "            itf_wav = waveforms[j]\n",
    "            trg_spk = current_speaker\n",
    "            itf_spk = speakers[j]\n",
    "            yield trg_wav, itf_wav, trg_spk, itf_spk\n",
    "\n",
    "def mix_speakers_by_snr(waveforms, speakers, num_mix, snr):\n",
    "    generator = joint_different_speakers(waveforms, speakers, num_mix)\n",
    "    mixed_data = []\n",
    "    for trg_wav, itf_wav, trg_spk, itf_spk in tqdm(generator, total=len(speakers)*num_mix):\n",
    "        # Calculate the scale to mix two speakers based on fixed SNR\n",
    "        itf_spk_power = np.mean(np.square(trg_wav)) / (10**(snr/10))\n",
    "        scale = np.sqrt(itf_spk_power / np.mean(np.square(itf_wav)))\n",
    "\n",
    "        # Mix two speakers based on given snr\n",
    "        mix_wav = trg_wav + scale * itf_wav\n",
    "\n",
    "        mixed_data.append([mix_wav, itf_spk, trg_spk])\n",
    "    return mixed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c1e7f91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T13:13:44.487338Z",
     "start_time": "2021-12-22T13:13:44.427571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945d9f6d7090493a9ff0655cdbaab046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mixed_data = mix_speakers_by_snr(X, y, num_mix=2, snr=5)\n",
    "mix_wavs = np.stack([data[0] for data in mixed_data])\n",
    "itf_spks = np.stack([data[1] for data in mixed_data])\n",
    "trg_spks = np.stack([data[2] for data in mixed_data])\n",
    "X_mix_wavs = torch.tensor(mix_wavs, dtype=torch.float)\n",
    "\n",
    "y_onehot = np.zeros((len(mix_wavs), NUM_CLASSES), dtype=int)\n",
    "for idx, (itf_spk, trg_spk) in enumerate(zip(itf_spks, trg_spks)):\n",
    "    y_onehot[idx, itf_spk] = 1\n",
    "    y_onehot[idx, trg_spk] = 1\n",
    "y_onehot = torch.from_numpy(y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3619cdbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T13:13:45.732623Z",
     "start_time": "2021-12-22T13:13:45.724644Z"
    }
   },
   "outputs": [],
   "source": [
    "class SVD(nn.Module):\n",
    "    \"\"\"\n",
    "    Singular value decomposition layer\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> A = torch.rand(126, 100, 20).to('cuda')\n",
    "    >>> U, S, V = SVD(compute_uv=True)(A)\n",
    "    >>> A_ = torch.matmul(U, torch.matmul(S, V.transpose(-1, -2)))\n",
    "    >>> print(torch.dist(A_, A))\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SVD, self).__init__()\n",
    "    \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        A: [b, m, n]\n",
    "        \n",
    "        Outputs\n",
    "        -------\n",
    "        U: [b, m, n]\n",
    "        S: [b, n, n]\n",
    "        V: [b, n, n]\n",
    "        \"\"\"\n",
    "        return self.svd_(A)\n",
    "        \n",
    "    @staticmethod\n",
    "    def svd_(A):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        A: torch.FloatTensor\n",
    "            A tensor of shape [b, m, n].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        U: [b, m, n]\n",
    "        S: [b, n, n]\n",
    "        V: [b, n, n]\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        1. https://www.youtube.com/watch?v=pSbafxDHdgE&t=205s\n",
    "        2. https://www2.math.ethz.ch/education/bachelor/lectures/hs2014/other/linalg_INFK/svdneu.pdf\n",
    "        \"\"\"\n",
    "        ATA = torch.matmul(A.transpose(-1, -2), A)\n",
    "        lv, vv = torch.linalg.eig(ATA)\n",
    "        lv = lv.real\n",
    "        vv = vv.real\n",
    "        V = F.normalize(vv, dim=1)\n",
    "        S = torch.diag_embed(torch.sqrt(lv))\n",
    "        U = torch.matmul(torch.matmul(A, V), torch.inverse(S))\n",
    "        return U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b6f0f68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T13:15:46.686487Z",
     "start_time": "2021-12-22T13:15:46.652776Z"
    }
   },
   "outputs": [],
   "source": [
    "class SincConv(nn.Module):\n",
    "    \"\"\"This function implements SincConv (SincNet).\n",
    "    M. Ravanelli, Y. Bengio, \"Speaker Recognition from raw waveform with\n",
    "    SincNet\", in Proc. of  SLT 2018 (https://arxiv.org/abs/1808.00158)\n",
    "    Arguments\n",
    "    ---------\n",
    "    input_shape : tuple\n",
    "        The shape of the input. Alternatively use ``in_channels``.\n",
    "    in_channels : int\n",
    "        The number of input channels. Alternatively use ``input_shape``.\n",
    "    out_channels : int\n",
    "        It is the number of output channels.\n",
    "    kernel_size: int\n",
    "        Kernel size of the convolutional filters.\n",
    "    stride : int\n",
    "        Stride factor of the convolutional filters. When the stride factor > 1,\n",
    "        a decimation in time is performed.\n",
    "    dilation : int\n",
    "        Dilation factor of the convolutional filters.\n",
    "    padding : str\n",
    "        (same, valid, causal). If \"valid\", no padding is performed.\n",
    "        If \"same\" and stride is 1, output shape is the same as the input shape.\n",
    "        \"causal\" results in causal (dilated) convolutions.\n",
    "    padding_mode : str\n",
    "        This flag specifies the type of padding. See torch.nn documentation\n",
    "        for more information.\n",
    "    groups : int\n",
    "        This option specifies the convolutional groups. See torch.nn\n",
    "        documentation for more information.\n",
    "    bias : bool\n",
    "        If True, the additive bias b is adopted.\n",
    "    sample_rate : int,\n",
    "        Sampling rate of the input signals. It is only used for sinc_conv.\n",
    "    min_low_hz : float\n",
    "        Lowest possible frequency (in Hz) for a filter. It is only used for\n",
    "        sinc_conv.\n",
    "    min_low_hz : float\n",
    "        Lowest possible value (in Hz) for a filter bandwidth.\n",
    "    Example\n",
    "    -------\n",
    "    >>> inp_tensor = torch.rand([10, 16000])\n",
    "    >>> conv = SincConv(input_shape=inp_tensor.shape, out_channels=25, kernel_size=11)\n",
    "    >>> out_tensor = conv(inp_tensor)\n",
    "    >>> out_tensor.shape\n",
    "    torch.Size([10, 16000, 25])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        input_shape=None,\n",
    "        in_channels=None,\n",
    "        stride=1,\n",
    "        dilation=1,\n",
    "        padding=\"same\",\n",
    "        padding_mode=\"reflect\",\n",
    "        sample_rate=16000,\n",
    "        min_low_hz=50,\n",
    "        min_band_hz=50,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.padding = padding\n",
    "        self.padding_mode = padding_mode\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_low_hz = min_low_hz\n",
    "        self.min_band_hz = min_band_hz\n",
    "\n",
    "        # input shape inference\n",
    "        if input_shape is None and in_channels is None:\n",
    "            raise ValueError(\"Must provide one of input_shape or in_channels\")\n",
    "\n",
    "        if in_channels is None:\n",
    "            in_channels = self._check_input_shape(input_shape)\n",
    "\n",
    "        # Initialize Sinc filters\n",
    "        self._init_sinc_conv()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns the output of the convolution.\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor (batch, time, channel)\n",
    "            input to convolve. 2d or 4d tensors are expected.\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, -1)\n",
    "        self.device = x.device\n",
    "\n",
    "        unsqueeze = x.ndim == 2\n",
    "        if unsqueeze:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        if self.padding == \"same\":\n",
    "            x = self._manage_padding(\n",
    "                x, self.kernel_size, self.dilation, self.stride\n",
    "            )\n",
    "\n",
    "        elif self.padding == \"causal\":\n",
    "            num_pad = (self.kernel_size - 1) * self.dilation\n",
    "            x = F.pad(x, (num_pad, 0))\n",
    "\n",
    "        elif self.padding == \"valid\":\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Padding must be 'same', 'valid' or 'causal'. Got %s.\"\n",
    "                % (self.padding)\n",
    "            )\n",
    "\n",
    "        sinc_filters = self._get_sinc_filters()\n",
    "\n",
    "        wx = F.conv1d(\n",
    "            x,\n",
    "            sinc_filters,\n",
    "            stride=self.stride,\n",
    "            padding=0,\n",
    "            dilation=self.dilation,\n",
    "        )\n",
    "\n",
    "        if unsqueeze:\n",
    "            wx = wx.squeeze(1)\n",
    "\n",
    "        wx = wx.transpose(1, -1)\n",
    "\n",
    "        return wx\n",
    "\n",
    "    def _check_input_shape(self, shape):\n",
    "        \"\"\"Checks the input shape and returns the number of input channels.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(shape) == 2:\n",
    "            in_channels = 1\n",
    "        elif len(shape) == 3:\n",
    "            in_channels = 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"sincconv expects 2d or 3d inputs. Got \" + str(len(shape))\n",
    "            )\n",
    "\n",
    "        # Kernel size must be odd\n",
    "        if self.kernel_size % 2 == 0:\n",
    "            raise ValueError(\n",
    "                \"The field kernel size must be an odd number. Got %s.\"\n",
    "                % (self.kernel_size)\n",
    "            )\n",
    "        return in_channels\n",
    "\n",
    "    def _get_sinc_filters(self,):\n",
    "        \"\"\"This functions creates the sinc-filters to used for sinc-conv.\n",
    "        \"\"\"\n",
    "        # Computing the low frequencies of the filters\n",
    "        low = self.min_low_hz + torch.abs(self.low_hz_)\n",
    "\n",
    "        # Setting minimum band and minimum freq\n",
    "        high = torch.clamp(\n",
    "            low + self.min_band_hz + torch.abs(self.band_hz_),\n",
    "            self.min_low_hz,\n",
    "            self.sample_rate / 2,\n",
    "        )\n",
    "        band = (high - low)[:, 0]\n",
    "\n",
    "        # Passing from n_ to the corresponding f_times_t domain\n",
    "        self.n_ = self.n_.to(self.device)\n",
    "        self.window_ = self.window_.to(self.device)\n",
    "        f_times_t_low = torch.matmul(low, self.n_)\n",
    "        f_times_t_high = torch.matmul(high, self.n_)\n",
    "\n",
    "        # Left part of the filters.\n",
    "        band_pass_left = (\n",
    "            (torch.sin(f_times_t_high) - torch.sin(f_times_t_low))\n",
    "            / (self.n_ / 2)\n",
    "        ) * self.window_\n",
    "\n",
    "        # Central element of the filter\n",
    "        band_pass_center = 2 * band.view(-1, 1)\n",
    "\n",
    "        # Right part of the filter (sinc filters are symmetric)\n",
    "        band_pass_right = torch.flip(band_pass_left, dims=[1])\n",
    "\n",
    "        # Combining left, central, and right part of the filter\n",
    "        band_pass = torch.cat(\n",
    "            [band_pass_left, band_pass_center, band_pass_right], dim=1\n",
    "        )\n",
    "\n",
    "        # Amplitude normalization\n",
    "        band_pass = band_pass / (2 * band[:, None])\n",
    "\n",
    "        # Setting up the filter coefficients\n",
    "        filters = band_pass.view(self.out_channels, 1, self.kernel_size)\n",
    "\n",
    "        return filters\n",
    "\n",
    "    def _init_sinc_conv(self):\n",
    "        \"\"\"Initializes the parameters of the sinc_conv layer.\"\"\"\n",
    "\n",
    "        # Initialize filterbanks such that they are equally spaced in Mel scale\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
    "\n",
    "        mel = torch.linspace(\n",
    "            self._to_mel(self.min_low_hz),\n",
    "            self._to_mel(high_hz),\n",
    "            self.out_channels + 1,\n",
    "        )\n",
    "\n",
    "        hz = self._to_hz(mel)\n",
    "\n",
    "        # Filter lower frequency and bands\n",
    "        self.low_hz_ = hz[:-1].unsqueeze(1)\n",
    "        self.band_hz_ = (hz[1:] - hz[:-1]).unsqueeze(1)\n",
    "\n",
    "        # Maiking freq and bands learnable\n",
    "        self.low_hz_ = nn.Parameter(self.low_hz_)\n",
    "        self.band_hz_ = nn.Parameter(self.band_hz_)\n",
    "\n",
    "        # Hamming window\n",
    "        n_lin = torch.linspace(\n",
    "            0, (self.kernel_size / 2) - 1, steps=int((self.kernel_size / 2))\n",
    "        )\n",
    "        self.window_ = 0.54 - 0.46 * torch.cos(\n",
    "            2 * math.pi * n_lin / self.kernel_size\n",
    "        )\n",
    "\n",
    "        # Time axis  (only half is needed due to symmetry)\n",
    "        n = (self.kernel_size - 1) / 2.0\n",
    "        self.n_ = (\n",
    "            2 * math.pi * torch.arange(-n, 0).view(1, -1) / self.sample_rate\n",
    "        )\n",
    "\n",
    "    def _to_mel(self, hz):\n",
    "        \"\"\"Converts frequency in Hz to the mel scale.\n",
    "        \"\"\"\n",
    "        return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "    def _to_hz(self, mel):\n",
    "        \"\"\"Converts frequency in the mel scale to Hz.\n",
    "        \"\"\"\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\n",
    "\n",
    "    def _manage_padding(\n",
    "        self, x, kernel_size: int, dilation: int, stride: int,\n",
    "    ):\n",
    "        \"\"\"This function performs zero-padding on the time axis\n",
    "        such that their lengths is unchanged after the convolution.\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : torch.Tensor\n",
    "            Input tensor.\n",
    "        kernel_size : int\n",
    "            Size of kernel.\n",
    "        dilation : int\n",
    "            Dilation used.\n",
    "        stride : int\n",
    "            Stride.\n",
    "        \"\"\"\n",
    "\n",
    "        # Detecting input shape\n",
    "        L_in = x.shape[-1]\n",
    "\n",
    "        # Time padding\n",
    "        padding = get_padding_elem(L_in, stride, kernel_size, dilation)\n",
    "\n",
    "        # Applying padding\n",
    "        x = F.pad(x, padding, mode=self.padding_mode)\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_padding_elem(L_in: int, stride: int, kernel_size: int, dilation: int):\n",
    "    \"\"\"This function computes the number of elements to add for zero-padding.\n",
    "    Arguments\n",
    "    ---------\n",
    "    L_in : int\n",
    "    stride: int\n",
    "    kernel_size : int\n",
    "    dilation : int\n",
    "    \"\"\"\n",
    "    if stride > 1:\n",
    "        n_steps = math.ceil(((L_in - kernel_size * dilation) / stride) + 1)\n",
    "        L_out = stride * (n_steps - 1) + kernel_size * dilation\n",
    "        padding = [kernel_size // 2, kernel_size // 2]\n",
    "\n",
    "    else:\n",
    "        L_out = (L_in - dilation * (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "        padding = [(L_in - L_out) // 2, (L_in - L_out) // 2]\n",
    "    return padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b5064c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T13:16:13.290243Z",
     "start_time": "2021-12-22T13:16:13.272370Z"
    }
   },
   "outputs": [],
   "source": [
    "class DemixingNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DemixingNet, self).__init__()\n",
    "        self.conv = SincConv(in_channels=1, out_channels=20, kernel_size=11)\n",
    "        \n",
    "    def forward(self, wavs):\n",
    "        return self.conv(wavs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "74c89d4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T13:16:13.725739Z",
     "start_time": "2021-12-22T13:16:13.636059Z"
    }
   },
   "outputs": [],
   "source": [
    "output = DemixingNet()(X_mix_wavs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77863d3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-22T13:16:13.990392Z",
     "start_time": "2021-12-22T13:16:13.985428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 16000, 20])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad59b20e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
